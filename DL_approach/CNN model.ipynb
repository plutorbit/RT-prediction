{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "from scipy.stats import gaussian_kde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:/Users/admin/OneDrive/Dokumenty/CNN/dataset_paper_f_cleaned_minmax.csv')\n",
    "#dataset_paper_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_normalized = dataset.min()\n",
    "max_normalized = dataset.max()\n",
    "print(f\"Range of normalized retention time: {min_normalized} to {max_normalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneHot Encoding \n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "unique_chars = list(set(''.join(dataset['sequence'])))\n",
    "one_hot_encoder.fit(np.array(unique_chars).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    one_hot_encoder.transform(np.array(list(seq)).reshape(-1, 1)) for seq in dataset['sequence']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(encoded_sequences, maxlen=58, padding='post', dtype='float32') \n",
    "y = dataset['retention_time'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_seq, X_test_seq, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data = np.array(X)\n",
    "sequence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(sequence_data)\n",
    "y=np.array(dataset['retention_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq, X_test_seq, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train).reshape(-1)\n",
    "y_test = np.array(y_test).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original_global = None\n",
    "y_test_original_global = None\n",
    "\n",
    "def objective(trial):\n",
    "    global y_pred_original_global, y_test_original_global  \n",
    "\n",
    "    r_number = 42\n",
    "    np.random.seed(r_number)\n",
    "    tf.random.set_seed(r_number)\n",
    "    random.seed(r_number)\n",
    "\n",
    "    X_train_seq, X_test_seq, y_train, y_test = train_test_split(\n",
    "         x, y, test_size=0.2, random_state=r_number\n",
    "    )   \n",
    "\n",
    "    min_value = dataset['retention_time'].min()\n",
    "    max_value = dataset['retention_time'].max()\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    filters = trial.suggest_categorical('filters', [32, 64, 128, 256])\n",
    "    kernel_size = trial.suggest_categorical('kernel_size', [3, 5, 7])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    units = trial.suggest_int('units', 32, 128, step=32)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    optimizer = {\n",
    "        'adam': Adam(learning_rate=learning_rate),\n",
    "        'sgd': SGD(learning_rate=learning_rate),\n",
    "        'rmsprop': RMSprop(learning_rate=learning_rate)\n",
    "    }[optimizer_name]\n",
    "\n",
    "    # Building the model\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv1D(filters=filters * 4, kernel_size=kernel_size, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=units, activation='relu'),\n",
    "        Dropout(rate=dropout_rate),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    # Training the model for Optuna Study\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train,\n",
    "        validation_data=(X_test_seq, y_test),\n",
    "        epochs=2, \n",
    "        batch_size=min(batch_size, len(X_train_seq)), \n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_loss, val_mae = model.evaluate(X_test_seq, y_test, verbose=0)\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "\n",
    "    y_pred_original_global = denormalize(y_pred, min_value, max_value)  \n",
    "    y_test_original_global = denormalize(y_test, min_value, max_value)\n",
    "\n",
    " \n",
    "    print(\"Inside objective - y_pred_original_global:\", y_pred_original_global[:5].flatten())\n",
    "    print(\"Inside objective - y_test_original_global:\", y_test_original_global[:5].flatten())\n",
    "\n",
    "    return mean_absolute_error(y_test_original_global, y_pred_original_global)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna study\n",
    "study = optuna.create_study(direction='minimize') \n",
    "study.optimize(objective, n_trials=50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_params\n",
    "\n",
    "best_model = Sequential([\n",
    "    \n",
    "    #Using best parameters\n",
    "    Conv1D(filters=best_trial['filters'], kernel_size=best_trial['kernel_size'], activation='relu', input_shape=(58, 20)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv1D(filters=best_trial['filters']*2, kernel_size=best_trial['kernel_size'], activation='relu', input_shape=(58, 20)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv1D(filters=best_trial['filters']*4, kernel_size=best_trial['kernel_size'], activation='relu', input_shape=(58, 20)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(units=best_trial['units'], activation='relu'),\n",
    "    Dropout(rate=best_trial['dropout_rate']),\n",
    "    Dense(1, activation='relu')\n",
    "])\n",
    "best_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=best_trial['learning_rate']),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "\n",
    "best_model.build(input_shape=(None, 58,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
    "print(\"Model input shape:\", best_model.input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         \n",
    "    patience=10,                 \n",
    "    restore_best_weights=True,  \n",
    "    min_delta=0.00001           \n",
    ")\n",
    "\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]  \n",
    ")\n",
    "\n",
    "test_loss, test_mae = best_model.evaluate(X_test_seq, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"final_model.h5\")\n",
    "print(\"Final model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[['loss', 'val_loss']].plot(figsize=(8, 5))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Predicted vs Actual Values -> Normalized\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6, label='Predicted vs. Actual')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Ideal Fit') \n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(\"final_model.h5\")\n",
    "\n",
    "y_pred = best_model.predict(X_test_seq)\n",
    "print(\"Raw y_pred min:\", y_pred.min(), \"Raw y_pred max:\", y_pred.max())\n",
    "\n",
    "y_pred_clipped = np.clip(y_pred, 0, 1)\n",
    "\n",
    "# Denormalized scale\n",
    "original_min = 0.0343385\n",
    "original_max = 60.026\n",
    "\n",
    "y_pred_original = y_pred_clipped * (original_max - original_min) + original_min\n",
    "\n",
    "global y_pred_original_global\n",
    "y_pred_original_global = y_pred_original\n",
    "\n",
    "print(\"First few denormalized predictions:\", y_pred_original_global[:5].flatten())\n",
    "print(\"Denormalized Predictions Range: Min =\", y_pred_original_global.min(), \"Max =\", y_pred_original_global.max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(\"final_model.h5\", compile=True)\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.001454220582305921)\n",
    "best_model.compile(optimizer=optimizer, loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "\n",
    "_ = best_model.predict(X_test_seq[:1])\n",
    "\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "\n",
    "y_test_original_global = np.array(y_test_original_global).flatten()\n",
    "\n",
    "original_min, original_max = 0.0343385, 60.026\n",
    "if y_test_original_global.max() > 1.0:\n",
    "    print(\"`y_test_original_global` is in the original scale\")\n",
    "else:\n",
    "    print(\"`y_test_original_global` seems to be normalized. Fixing it...\")\n",
    "    y_test_original_global = y_test_original_global * (original_max - original_min) + original_min\n",
    "    print(\"Applied correct denormalization.\")\n",
    "\n",
    "\n",
    "y_pred_clipped = np.clip(y_pred, 0, 1) \n",
    "y_pred_original_global = y_pred_clipped * (original_max - original_min) + original_min\n",
    "\n",
    "# PLot\n",
    "min_value = min(y_test_original_global.min(), y_pred_original_global.min())\n",
    "max_value = max(y_test_original_global.max(), y_pred_original_global.max())\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test_original_global, y_pred_original_global, alpha=0.6, label=\"Predicted vs. Actual\", s=10)\n",
    "plt.plot([min_value, max_value], [min_value, max_value], 'r--', label=\"Ideal Fit\")\n",
    "\n",
    "plt.xlabel(\"Actual Retention Time\")\n",
    "plt.ylabel(\"Predicted Retention Time\")\n",
    "plt.title(\"Predicted vs. Actual Retention Times\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(normalized_values, min_value, max_value):\n",
    "    return normalized_values * (max_value - min_value) + min_value\n",
    "\n",
    "original_min = 0.0343385\n",
    "original_max = 60.026\n",
    "\n",
    "y_train_pred = best_model.predict(X_train_seq)\n",
    "y_test_pred = best_model.predict(X_test_seq)\n",
    "\n",
    "y_train_pred_clipped = np.clip(y_train_pred, 0.0, 1.0)\n",
    "y_test_pred_clipped = np.clip(y_test_pred, 0.0, 1.0)\n",
    "\n",
    "y_train_pred_denorm = denormalize(y_train_pred_clipped, original_min, original_max)\n",
    "y_test_pred_denorm = denormalize(y_test_pred_clipped, original_min, original_max)\n",
    "y_train_denorm = denormalize(y_train, original_min, original_max)\n",
    "y_test_denorm = denormalize(y_test, original_min, original_max)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_denorm, y_train_pred_denorm)\n",
    "train_mae = mean_absolute_error(y_train_denorm, y_train_pred_denorm)\n",
    "train_medae = median_absolute_error(y_train_denorm, y_train_pred_denorm)\n",
    "train_r2 = r2_score(y_train_denorm, y_train_pred_denorm)\n",
    "\n",
    "test_mse = mean_squared_error(y_test_denorm, y_test_pred_denorm)\n",
    "test_mae = mean_absolute_error(y_test_denorm, y_test_pred_denorm)\n",
    "test_medae = median_absolute_error(y_test_denorm, y_test_pred_denorm)\n",
    "test_r2 = r2_score(y_test_denorm, y_test_pred_denorm)\n",
    "\n",
    "print(\"**Final Metrics on Denormalized Data**\")\n",
    "\n",
    "print(\"\\n Train Metrics:\")\n",
    "print(f\"  MSE: {train_mse:.4f}\")\n",
    "print(f\"  MAE: {train_mae:.4f}\")\n",
    "print(f\"  MedAE: {train_medae:.4f}\")\n",
    "print(f\"  R²: {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\n Test Metrics:\")\n",
    "print(f\"  MSE: {test_mse:.4f}\")\n",
    "print(f\"  MAE: {test_mae:.4f}\")\n",
    "print(f\"  MedAE: {test_medae:.4f}\")\n",
    "print(f\"  R²: {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n **Verification of Predictions**\")\n",
    "print(f\"Train Predictions Range: {y_train_pred_denorm.min()} to {y_train_pred_denorm.max()}\")\n",
    "print(f\"Test Predictions Range: {y_test_pred_denorm.min()} to {y_test_pred_denorm.max()}\")\n",
    "print(f\"Expected Range: {original_min} to {original_max}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_test' not in globals() or y_test.size == 0:\n",
    "    y_test = np.linspace(0, 1, 89490) \n",
    "if 'y_pred' not in globals() or y_pred.size == 0:\n",
    "    y_pred = np.linspace(0, 1, 89490) + np.random.normal(0, 0.05, 89490)\n",
    "\n",
    "y_test = np.ravel(y_test)\n",
    "y_pred = np.ravel(y_pred)\n",
    "\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"y_pred shape:\", y_pred.shape)\n",
    "print(\"y_test (normalized) range:\", y_test.min(), y_test.max())\n",
    "print(\"y_pred (normalized) range:\", y_pred.min(), y_pred.max())\n",
    "\n",
    "assert len(y_test) == len(y_pred)\n",
    "\n",
    "y_test_denormalized = denormalize(y_test, original_min, original_max)\n",
    "y_pred_denormalized = denormalize(y_pred, original_min, original_max)\n",
    "\n",
    "print(\"y_test_denormalized range:\", y_test_denormalized.min(), y_test_denormalized.max())\n",
    "print(\"y_pred_denormalized range:\", y_pred_denormalized.min(), y_pred_denormalized.max())\n",
    "\n",
    "xy = np.vstack([y_test_denormalized, y_pred_denormalized])\n",
    "z = gaussian_kde(xy)(xy) \n",
    " \n",
    "#Plot \n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(\n",
    "    y_test_denormalized,\n",
    "    y_pred_denormalized,\n",
    "    c=z,\n",
    "    s=10, \n",
    "    cmap='viridis', \n",
    "    alpha=0.7,\n",
    "    label='Predictions'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    [original_min, original_max],\n",
    "    [original_min, original_max],\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    label='Ideal Fit'\n",
    ")\n",
    "\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.xlabel('Actual Values (Retention Time)')\n",
    "plt.ylabel('Predicted Values (Retention Time)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.colorbar(label='Density') \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_denormalized = denormalize(y_test, original_min, original_max)\n",
    "y_pred_denormalized = denormalize(y_pred, original_min, original_max)\n",
    "\n",
    "y_test_denormalized = np.ravel(y_test_denormalized)\n",
    "y_pred_denormalized = np.ravel(y_pred_denormalized)\n",
    "\n",
    "assert y_test_denormalized.shape == y_pred_denormalized.shape, (\n",
    "    f\"Shape mismatch: y_test_denormalized {y_test_denormalized.shape}, \"\n",
    "    f\"y_pred_denormalized {y_pred_denormalized.shape}\"\n",
    ")\n",
    "\n",
    "absolute_errors = np.abs(y_test_denormalized - y_pred_denormalized)\n",
    "\n",
    "medae = np.median(absolute_errors)\n",
    "mae = np.mean(absolute_errors)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(absolute_errors, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.axvline(medae, color='r', linestyle='--', label='Median Absolute Error')\n",
    "plt.axvline(mae, color='g', linestyle='--', label='Mean Absolute Error')\n",
    "plt.title('Distribution of Absolute Errors (Denormalized)')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "mean_pred = np.mean(y_test)\n",
    "y_baseline_mean = np.full_like(y_test, mean_pred)\n",
    "\n",
    "median_pred = np.median(y_test)\n",
    "y_baseline_median = np.full_like(y_test, median_pred)\n",
    "\n",
    "min_value = np.min(y_test)\n",
    "max_value = np.max(y_test)\n",
    "y_baseline_random = np.random.uniform(min_value, max_value, size=len(y_test))\n",
    "\n",
    "cnn_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "baseline_mean_mae = mean_absolute_error(y_test, y_baseline_mean)\n",
    "baseline_median_mae = mean_absolute_error(y_test, y_baseline_median)\n",
    "baseline_random_mae = mean_absolute_error(y_test, y_baseline_random)\n",
    "\n",
    "print(f\"CNN Model MAE: {cnn_mae:.3f}\")\n",
    "print(f\"Baseline Mean Prediction MAE: {baseline_mean_mae:.3f}\")\n",
    "print(f\"Baseline Median Prediction MAE: {baseline_median_mae:.3f}\")\n",
    "print(f\"Baseline Random Prediction MAE: {baseline_random_mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.ravel(y_test)\n",
    "y_test_pred = np.ravel(y_test_pred)\n",
    "\n",
    "mean_pred = np.mean(y_test)\n",
    "y_baseline_mean = np.full_like(y_test, mean_pred)\n",
    "\n",
    "cnn_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "cnn_mse = mean_squared_error(y_test, y_test_pred)\n",
    "cnn_medae = np.median(np.abs(y_test - y_test_pred))  \n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test, y_baseline_mean)\n",
    "baseline_mse = mean_squared_error(y_test, y_baseline_mean)\n",
    "baseline_medae = np.median(np.abs(y_test - y_baseline_mean))  \n",
    "\n",
    "metrics = ['MAE', 'MSE', 'MedAE']\n",
    "cnn_values = [cnn_mae, cnn_mse, cnn_medae]\n",
    "baseline_values = [baseline_mae, baseline_mse, baseline_medae]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35 \n",
    "\n",
    "\n",
    "cnn_color = '#5b0772'  \n",
    "baseline_color = '#f4c2c2'  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars1 = plt.bar(x - width / 2, cnn_values, width, label='CNN Model', color=cnn_color)\n",
    "bars2 = plt.bar(x + width / 2, baseline_values, width, label='Baseline', color=baseline_color)\n",
    "\n",
    "\n",
    "for bar in bars1:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{bar.get_height():.2f}', ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{bar.get_height():.2f}', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylabel('Scores')\n",
    "plt.legend()\n",
    "plt.ylim(0, max(max(cnn_values), max(baseline_values)) + 0.1) \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
